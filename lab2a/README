NAME: Robathan Harries
EMAIL: r0b@ucla.edu
ID: 904836501
SLIPDAYS: 0

QUESTIONS
2.1.1:
Since the critical add section takes up only a small fraction of a thread's running time, each operation only has a small chance of being interrupted in the right way to cause an incorrect result. Thus, it takes a lot of iterations for that small chance to accumulate enough for the end result to be reliably incorrect. On the other hand, a small number of iterations will rarely fail, since the critical section window is so small that the chances of multiple threads entering it at once are pretty small.

2.1.2:
The yield option causes a context switch for every single operation done, whereas under normal circumstances a thread should be able to do multiple operations within one time slice. These frequent context switches take up all this extra time. Since our implementation doesn't care about whether the time it quotes is due to OS-controlled context switches or thread operations, we count indiscriminately.

2.1.3:
Our program incurs a fair amount of overhead when it initializes, parses options, and creates threads. This overhead is included in the time measurement, and thus included in our average time per operation measurement. However, as the number of iterations increases and the runtime of the program increases overall, this fixed overhead has less and less of an effect. Thus, if we increased the number of iterations to infinity, the cost per operation would level out to its 'true' level. Obviously this isn't quite feasible, so we have to settle for as many iterations as we can feasibly compute.

2.1.4:
Our protection methods have almost no effect on runtime unless multiple threads attempt to enter the critical add section at the same time. With a low number of threads, the chances of multiple threads trying to enter the small critical window of the loop at the same time is very low, so the protection methods aren't used much and the program runs very similarly to the unprotected program. As we increase the number of threads, race conditions will occur far more frequently, and protection methods will force threads to stall, increasing the overall runtime of the program, and thus the cost per operation. The unprotected program, on the other hand, will never stall threads when they try to access the shared variable at the same time.

2.2.1:
In part 1, the time per mutex-protected operation increased linearly with a small slope, and seemed level off at around 8-12 threads. In part 2, the time per mutex-protected operation was consistent, and didn't increase with the number of threads. The reason these are generally level is because attempting to lock a locked mutex will cause a thread to yield, and thus very few CPU cycles are wasted.

2.2.2:
In part 1, the time per spinlock-protected operation increased linearly, but didn't level off like the mutex-protected case. However, in part 2 the time per spinlock-protected operation was just as constant as in the mutex-protected case for part 2. This is a big surprise, and I don't know how to explain it, since I would expect a spinlock protection to cause increasing delays as the number of threads increased, since more threads would be wasting CPU cycles while busy waiting.







